---
title: "Autonomous NVMe TCP offload"
collection: talks
permalink: /talks/2021-nvme-tcp
type: "Talk"
excerpt: ''
date: 2021-07-20
venue: "netdev 0x15, Proceedings of NetDev 0x15: The Technical Conference on Linux Networking"
location: "Virtual"
extlink: 'https://netdevconf.info/0x15/session.html?Autonomous-NVMe-TCP-offload'
slides: '/files/2021-netdev_0x15_nvme-tcp.pdf'
#citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
authors: Boris Pismenny, Yoray Zack, Ben Ben-Ishay, and Or Gerlitz
---

NVMe-TCP is a high-performance pipelined storage protocol over TCP which
abstracts remote access to a storage controller, providing hosts with the
illusion of local storage. In NVMe-TCP, each storage queue is mapped to a TCP
socket. Read and write IOs operations are translated to RPC operations. Each
operation has a unique identifier, called capsule identifier (CID) and servers
can handle CIDs out of order to allow small IOs to bypass large IOs and improve
performance. Additionally, each RPC is protected by application-layer CRC
generated by senders and verified on receivers.

Traditional approaches to offload NVMe-TCP require offloading all layer-4
functionality: TCP, IP, routing, QoS, NAT, firewall, tunneling, etc. This is
undesirable as all of these are complex, and their overhead on bulky storage
operations is easily mitigated using batching as demonstrated last year in
"NVMe-over-TCP â‰ˆ NVMe-over-RDMA".

In this talk, we will present how we offload CPU-intensive operations that
cannot be optimized away using batching or clever software engineering: copy
and CRC. Our offload is independent of layer-4 functionality, i.e. we offload
NVMe-TCP autonomously. The main challenges addressed with our approach:
(1) handling retransmission and reordering; (2) offloading transparently to
software TCP/IP.
